{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import xmltodict\n",
    "def clean_data(text):\n",
    "    text = text.replace('{vocalsound}', '')\n",
    "    text = text.replace('{disfmarker}', '')\n",
    "    text = text.replace('a_m_i_', 'ami')\n",
    "    text = text.replace('l_c_d_', 'lcd')\n",
    "    text = text.replace('p_m_s', 'pms')\n",
    "    text = text.replace('t_v_', 'tv')\n",
    "    text = text.replace('{pause}', '')\n",
    "    text = text.replace('{nonvocalsound}', '')\n",
    "    text = text.replace('{gap}', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_dict = {\n",
    "    'dataset': [],\n",
    "    'file_id': [],\n",
    "    'text': [],\n",
    "    'summary': []\n",
    "}\n",
    "\n",
    "for dataset in os.listdir('../data/KeywordExtractor-Datasets/datasets/'):\n",
    "    path = os.path.join('../data/KeywordExtractor-Datasets/datasets/', dataset)\n",
    "    for txt_file in os.listdir(os.path.join(path, 'docsutf8')):\n",
    "        file_id = txt_file.replace('.txt', '')\n",
    "        try:\n",
    "            keyword_dict['dataset'].append(dataset)\n",
    "            keyword_dict['file_id'].append(file_id)\n",
    "            \n",
    "            with open(os.path.join(path, 'docsutf8', txt_file)) as f:\n",
    "                lines = f.readlines()\n",
    "                striped_txt = []\n",
    "                for line in lines:\n",
    "                    striped_txt.append(line.strip())\n",
    "                keyword_dict['text'].append(''.join(striped_txt))\n",
    "            \n",
    "            with open(os.path.join(path, 'keys', f'{file_id}.key')) as f:\n",
    "                lines = f.readlines()\n",
    "                striped_keys = []\n",
    "                for line in lines:\n",
    "                    striped_keys.append(line.strip())\n",
    "                keyword_dict['summary'].append(','.join(striped_keys))\n",
    "        except:\n",
    "            print(f'could not process file: {txt_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(keyword_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train.to_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/keyword-extraction/train.csv', index=False)\n",
    "test.to_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/keyword-extraction/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /Users/silas.rudolf/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.94M/2.94M [00:00<00:00, 7.94MB/s]\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset samsum downloaded and prepared to /Users/silas.rudolf/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 205.04it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"samsum\")\n",
    "dataset_kw = load_dataset(\"51la5/keyword-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kw = dataset_kw['train'].to_pandas().append(dataset_kw['test'].to_pandas())\n",
    "df_kw['type'] = 'KEYWORD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = dataset['train'].to_pandas().append(dataset['validation'].to_pandas()).append(dataset['test'].to_pandas())\n",
    "df_sum['type'] = 'KEYPHRASE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qm_(path):\n",
    "    keyword_dict = {\n",
    "        'dataset': [],\n",
    "        'file_id': [],\n",
    "        'text': [],\n",
    "        'summary': [],\n",
    "        'type': []\n",
    "    }\n",
    "    for f in os.listdir(path):\n",
    "        with open(os.path.join(path, f)) as t:\n",
    "            transcript = json.load(t)\n",
    "\n",
    "            for topic in transcript['topic_list']:\n",
    "                keyword_dict['dataset'].append('QMSum')\n",
    "                keyword_dict['file_id'].append(f)\n",
    "\n",
    "                span_a, span_b = topic['relevant_text_span'][0]\n",
    "                spans = transcript['meeting_transcripts'][int(span_a): int(span_b)]\n",
    "                text = []\n",
    "                for utterance in spans:\n",
    "                    text.append(f\"{utterance['speaker']}: {clean_data(utterance['content'])}\")\n",
    "                keyword_dict['text'].append(''.join(text))\n",
    "                keyword_dict['summary'].append(topic['topic'])\n",
    "                keyword_dict['type'].append('KEYPHRASE')\n",
    "\n",
    "    return pd.DataFrame(keyword_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '/Users/silas.rudolf/projects/School/MA/experiments/data/QMSum/data/ALL/train'\n",
    "path_val = '/Users/silas.rudolf/projects/School/MA/experiments/data/QMSum/data/ALL/val'\n",
    "path_test = '/Users/silas.rudolf/projects/School/MA/experiments/data/QMSum/data/ALL/test'\n",
    "\n",
    "qm_t = qm_(path_train)\n",
    "qm_val = qm_(path_val)\n",
    "qm_test = qm_(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm_df = qm_t.append(qm_val).append(qm_test)\n",
    "df_sum.rename(columns={'id': 'file_id', 'dialogue': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_kw.append(df_sum).append(qm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train = train.drop(train[train.text == ''].index).dropna(how='all')\n",
    "test = test.drop(test[test.text == ''].index).dropna(how='all')\n",
    "\n",
    "train.to_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/keyword-extraction/train.csv', index=False)\n",
    "test.to_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/keyword-extraction/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration 51la5--keyword-extraction-e9f2eb93d6495740\n",
      "Reusing dataset csv (/Users/silas.rudolf/.cache/huggingface/datasets/51la5___csv/51la5--keyword-extraction-e9f2eb93d6495740/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 2/2 [00:00<00:00, 74.45it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"51la5/keyword-extraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/silas.rudolf/projects/School/MA/experiments/data/ake-datasets/datasets/ACM/test/1005058.xml\") as xml_file:\n",
    "    data_dict = xmltodict.parse(xml_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ake_(path, prefix):\n",
    "    keyword_dict = {\n",
    "        'dataset': [],\n",
    "        'file_id': [],\n",
    "        'text': [],\n",
    "        'summary': [],\n",
    "        'type': []\n",
    "    }\n",
    "    with open(os.path.join(path, f'references/{prefix}.combined.json')) as f:\n",
    "        references = json.load(f)\n",
    "\n",
    "    for filename in os.listdir(os.path.join(path, f'{prefix}')):\n",
    "        file_id = filename.replace('.xml', '')\n",
    "        with open(os.path.join(path, f'{prefix}', filename)) as xml_file:\n",
    "            doc = xmltodict.parse(xml_file.read())\n",
    "\n",
    "        sentences = []\n",
    "        for sentence in doc['root']['document']['sentences']['sentence']:\n",
    "            try:\n",
    "                sentences.append(' '.join([i['word'] for i in sentence['tokens']['token']]))\n",
    "            except:\n",
    "                pass\n",
    "        keyword_dict['dataset'].append(path.split('/')[-1])\n",
    "        keyword_dict['file_id'].append(file_id)\n",
    "        keyword_dict['text'].append(''.join(sentences))\n",
    "        keyword_dict['summary'].append(','.join([i[0] for i in references[file_id]]))\n",
    "        keyword_dict['type'].append('KEYWORD')\n",
    "    return keyword_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ake_('/Users/silas.rudolf/projects/School/MA/experiments/data/ake-datasets/datasets/NUS', 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(t).to_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/ake-datasets/df5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/ake-datasets/df1.csv')\n",
    "df2 = pd.read_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/ake-datasets/df2.csv')\n",
    "df3 = pd.read_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/ake-datasets/df3.csv')\n",
    "df4 = pd.read_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/ake-datasets/df4.csv')\n",
    "df5 = pd.read_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/ake-datasets/df5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df1.append(df2).append(df3).append(df4).append(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration 51la5--keyword-extraction-e9f2eb93d6495740\n",
      "Reusing dataset csv (/Users/silas.rudolf/.cache/huggingface/datasets/51la5___csv/51la5--keyword-extraction-e9f2eb93d6495740/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 2/2 [00:00<00:00, 175.49it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"51la5/keyword-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_datasets['train'].to_pandas().append(raw_datasets['test'].to_pandas()).append(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 37.05ba/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 41.74ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 43.58ba/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 50.26ba/s]\n"
     ]
    }
   ],
   "source": [
    "x = raw_datasets.filter(lambda x: x['type'] == 'KEYWORD')\n",
    "y = raw_datasets.filter(lambda x: x['type'] == 'KEYPHRASE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwords = [i for i in x['train']['text']]\n",
    "kphrase = [i for i in y['train']['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x['test'].to_pandas()['dataset'] == None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>file_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>13680597</td>\n",
       "      <td>Jimmy: Sorry, have you seen Maria today?\\r\\nPe...</td>\n",
       "      <td>Peter saw Maria briefly today and he thinks sh...</td>\n",
       "      <td>KEYPHRASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schutz2008</td>\n",
       "      <td>Arch_Orthop_Trauma_Surg-4-1-2225998</td>\n",
       "      <td>Arch Orthop Trauma SurgArchives of Orthopaedic...</td>\n",
       "      <td>Classification,Radius,Radius fracture,X-ray,Di...</td>\n",
       "      <td>KEYWORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>13864482</td>\n",
       "      <td>Eva: Michelle has a huge stain on her bum, has...</td>\n",
       "      <td>Michelle has a stain on her bum. Harriett will...</td>\n",
       "      <td>KEYPHRASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>13727696</td>\n",
       "      <td>Daniel: beer?\\r\\nDavid: oh, man, sure\\r\\nDanie...</td>\n",
       "      <td>Daniel will bring some beers for himself and D...</td>\n",
       "      <td>KEYPHRASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>13716006</td>\n",
       "      <td>Faith: Everybody, look, Lottie's back! :D\\r\\nL...</td>\n",
       "      <td>Lottie went on a date with Tommy but is not su...</td>\n",
       "      <td>KEYPHRASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5508</th>\n",
       "      <td>None</td>\n",
       "      <td>13682232</td>\n",
       "      <td>Anna: &lt;file_video&gt;\\r\\nKate: :-) New song?\\r\\nA...</td>\n",
       "      <td>Anna is crazy about the new song.</td>\n",
       "      <td>KEYPHRASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5509</th>\n",
       "      <td>None</td>\n",
       "      <td>13731478</td>\n",
       "      <td>Mary: don't forget to pick up your sister!\\r\\n...</td>\n",
       "      <td>Freddie will pick up his sister. Mary will buy...</td>\n",
       "      <td>KEYPHRASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510</th>\n",
       "      <td>SemEval2010</td>\n",
       "      <td>J-55</td>\n",
       "      <td>From Optimal Limited To Unlimited Supply Aucti...</td>\n",
       "      <td>mechanism design,auction,competitive analysis,...</td>\n",
       "      <td>KEYWORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5511</th>\n",
       "      <td>None</td>\n",
       "      <td>13729357</td>\n",
       "      <td>Dot: Hi Jimmy, I have a favour to ask\\r\\nJimmy...</td>\n",
       "      <td>Ted was supposed to take care of Dot's cats un...</td>\n",
       "      <td>KEYPHRASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5512</th>\n",
       "      <td>None</td>\n",
       "      <td>13681874</td>\n",
       "      <td>Mom: clean your room!\\r\\nDerek: later\\r\\nMom: ...</td>\n",
       "      <td>Mom wants Derek to clean his room now and in t...</td>\n",
       "      <td>KEYPHRASE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5513 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          dataset                              file_id  \\\n",
       "0            None                             13680597   \n",
       "1      Schutz2008  Arch_Orthop_Trauma_Surg-4-1-2225998   \n",
       "2            None                             13864482   \n",
       "3            None                             13727696   \n",
       "4            None                             13716006   \n",
       "...           ...                                  ...   \n",
       "5508         None                             13682232   \n",
       "5509         None                             13731478   \n",
       "5510  SemEval2010                                 J-55   \n",
       "5511         None                             13729357   \n",
       "5512         None                             13681874   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Jimmy: Sorry, have you seen Maria today?\\r\\nPe...   \n",
       "1     Arch Orthop Trauma SurgArchives of Orthopaedic...   \n",
       "2     Eva: Michelle has a huge stain on her bum, has...   \n",
       "3     Daniel: beer?\\r\\nDavid: oh, man, sure\\r\\nDanie...   \n",
       "4     Faith: Everybody, look, Lottie's back! :D\\r\\nL...   \n",
       "...                                                 ...   \n",
       "5508  Anna: <file_video>\\r\\nKate: :-) New song?\\r\\nA...   \n",
       "5509  Mary: don't forget to pick up your sister!\\r\\n...   \n",
       "5510  From Optimal Limited To Unlimited Supply Aucti...   \n",
       "5511  Dot: Hi Jimmy, I have a favour to ask\\r\\nJimmy...   \n",
       "5512  Mom: clean your room!\\r\\nDerek: later\\r\\nMom: ...   \n",
       "\n",
       "                                                summary       type  \n",
       "0     Peter saw Maria briefly today and he thinks sh...  KEYPHRASE  \n",
       "1     Classification,Radius,Radius fracture,X-ray,Di...    KEYWORD  \n",
       "2     Michelle has a stain on her bum. Harriett will...  KEYPHRASE  \n",
       "3     Daniel will bring some beers for himself and D...  KEYPHRASE  \n",
       "4     Lottie went on a date with Tommy but is not su...  KEYPHRASE  \n",
       "...                                                 ...        ...  \n",
       "5508                  Anna is crazy about the new song.  KEYPHRASE  \n",
       "5509  Freddie will pick up his sister. Mary will buy...  KEYPHRASE  \n",
       "5510  mechanism design,auction,competitive analysis,...    KEYWORD  \n",
       "5511  Ted was supposed to take care of Dot's cats un...  KEYPHRASE  \n",
       "5512  Mom wants Derek to clean his room now and in t...  KEYPHRASE  \n",
       "\n",
       "[5513 rows x 5 columns]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "path = '/Users/silas.rudolf/projects/School/MA/experiments/data/QMSum/data/ALL/test'\n",
    "files = os.listdir(path)\n",
    "\n",
    "keyphrases_dict = {\n",
    "    'filename': [],\n",
    "    'text': [],\n",
    "    'summary': []\n",
    "}\n",
    "\n",
    "for filename in files:\n",
    "    with open(os.path.join(path, filename)) as f:\n",
    "        json_file = json.load(f)\n",
    "        keyphrases_dict['filename'].append(filename)\n",
    "\n",
    "        text = []\n",
    "        for utterance in json_file['meeting_transcripts']:\n",
    "            text.append(f\"{utterance['speaker']}: {clean_data(utterance['content'])}\")\n",
    "        keyphrases_dict['text'].append(''.join(text))\n",
    "        keyphrases_dict['summary'].append(json_file['general_query_list'][0]['answer'])\n",
    "\n",
    "\n",
    "path = '/Users/silas.rudolf/projects/School/MA/experiments/data/QMSum/data/ALL/val'\n",
    "files = os.listdir(path)\n",
    "for filename in files:\n",
    "    with open(os.path.join(path, filename)) as f:\n",
    "        json_file = json.load(f)\n",
    "        keyphrases_dict['filename'].append(filename)\n",
    "\n",
    "        text = []\n",
    "        for utterance in json_file['meeting_transcripts']:\n",
    "            text.append(f\"{utterance['speaker']}: {clean_data(utterance['content'])}\")\n",
    "        keyphrases_dict['text'].append(''.join(text))\n",
    "        keyphrases_dict['summary'].append(json_file['general_query_list'][0]['answer'])\n",
    "\n",
    "path = '/Users/silas.rudolf/projects/School/MA/experiments/data/QMSum/data/ALL/train'\n",
    "files = os.listdir(path)\n",
    "for filename in files:\n",
    "    with open(os.path.join(path, filename)) as f:\n",
    "        json_file = json.load(f)\n",
    "        keyphrases_dict['filename'].append(filename)\n",
    "\n",
    "        text = []\n",
    "        for utterance in json_file['meeting_transcripts']:\n",
    "            text.append(f\"{utterance['speaker']}: {clean_data(utterance['content'])}\")\n",
    "        keyphrases_dict['text'].append(''.join(text))\n",
    "        keyphrases_dict['summary'].append(json_file['general_query_list'][0]['answer'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(keyphrases_dict)\n",
    "df.to_csv('/Users/silas.rudolf/projects/School/MA/experiments/data/keyphrase-eval/reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('locator_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d10268de14796ab72802f3d5204661607cb47f5caae6ef1d9733064b38c2714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
