{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer_keyword = AutoTokenizer.from_pretrained('/Users/silas.rudolf/projects/School/MA/experiments/data/keyword-extraction/checkpoint-keywords')\n",
    "model_keyword = AutoModelForSeq2SeqLM.from_pretrained('/Users/silas.rudolf/projects/School/MA/experiments/data/keyword-extraction/checkpoint-keywords')\n",
    "tokenizer_keyphrase = AutoTokenizer.from_pretrained('/Users/silas.rudolf/projects/School/MA/experiments/data/keyword-extraction/checkpoint-keyphrases')\n",
    "model_keyphrase = AutoModelForSeq2SeqLM.from_pretrained('/Users/silas.rudolf/projects/School/MA/experiments/data/keyword-extraction/checkpoint-keyphrases')\n",
    "\n",
    "# ARTICLE_TO_SUMMARIZE = (\n",
    "#     \" Okay. It's now being recorded. So then let's get started  and start with Ivan.  Yes, good morning everybody.  So, yesterday,  I have started working on the  implementation.  Nothing started, but continued working on implementation on gauge icon.  I had small discussion with Philip about some obstacles there.  I would need some input from you Marco, or you Flavia.  I don't know if you  guys, catch up the ticket.  And  after that,  we had like a pre-planning session  life  for the first time ever.  It was awesome. And yeah,  so today  I'm going to continue with the gauge icon and I have some ship  shipment to  probably do that in the second part of the day.  That's it.  Awesome.  Yep. And so that it's in there and mentioned you  guys Marco and Flavia on Gitlab  and in the thicket regarding the go check. Maybe Ivan can you shortly share  the link in development Channel  or so?  Just to keep these guys in the loop,  then we will continue with Djuradj.  Yes, hello, good morning again, so yesterday we had pre-planning and also took a look at the  house agency LinkedIn integration.  So for today, I plan to check  what we mentioned yesterday, the add analytics and  creative assets. So  to see do we have some title there? Or can we fetch some  different title there?  Yes, and then I will continue with the campaign export.  Alright,  perfect. Let me know if we should look into it.  The LinkedIn API worded together  and  so that we can prepare a feedback for the house by latest tomorrow  where we are. And if you have questions, especially in the naming, schema.  Good.  Thanks.  Then Silas.  Yes, good morning.  So yesterday I finished up all the remaining stuff  like ordering of the content  and  pushed everything in the evening to prod,  as some of you noticed  today. I'm basically done with everything.  We only have this interview.  I think right Phillip.  Got moved.  It got moved even  so  I can take up anything from the backlog.  Perfect.  Okay, then I will check the backlog as soon as my computer's back up  I' like to know  it.  Thanks  then Luca.  Good morning.  Yeah yesterday we had the interview with the Italian guy Armando  Alfredo  Alfredo. I'm  sorry.  Well,  yes,  very talky.  So very talkative guy.  Yeah, probably a bit  slow on all the rest.  but  whatever.  Then I finally finished the adjustment for Generali  so that we have the latest things can  think campaign in.  And  I'm starting working on this.  I mean, I starting at now that we do a larger screen. I can finally do this impression-share calculation with real spent instead of percentages.  Perfect  enjoy the large screen.  Yes.  All right,  so then I will give shortly update from my side and then I can  hold, I will hand over to customer  success  and Flavia and Marco. So from my side. Yeah, pre-planning, preparation  catched up with  All the discussions we had afterwards. So, development Channel  and I will check with Flavia and Marco today regarding priorities. I see some  top priority ones which are not in yet.  Let's see how this goes. So maybe there's still some updates coming and then we keep you posted  in development Channel.  Then for today.  I will try to catch up with Nexoya, local set up regarding limits  as this did not work when we  Rolled out  together in Belgrade.  And I will keep you posted on this and then we'll check with Silas on the backlog  and I think also witl regarding  and  I think he.. I  forgot  the lightness and Readiness check for get to get Gateway service,  which would also solve the issue. We faced yesterday late evening, when  you push changes on core graphql or Gateway service.  Yes,  that's it from my side. So then  Flavia.  Yes,  so yesterday,  lots of customer meetings. As I mentioned yesterday.  They all went really well.  They all love the dashboard and the insights that we gave to them. And so, scribble is now actually really happy which is great.  So, I went way,  way better than expected.  Yeah, I think that the rest of the week is actually quiet on customer success site.  Yep. Just wanting  one  thing that I have is trying to apply to cost,  but you already saw .  Making use of the new feature  right away.  That's correct.  Hope it works as expected.  Then Marco.  From My Sight,  I think Flavia's  totally underrating yesterdays day  right?  It was absolutely astonishing how scribble  was  like, excited  about what we showed them  crazy gave you showed  their problem, is they lost  conversions. in t last three weeks.  They didn't know why they assumed its Nexoya maybe even,  they lost like, 21% of conversions,  like, literally bottom line conversions big problem for them.  Big risk  and we made an analysis over all our clients and show that the average client is loosing 41 percent in the last  25 days  because of the  crockpot potentially because of the war situation, and  they are actually oa good track,  his quote was, I can now sleep again nicely  because I couldn't sleep last three weeks because this problem, I didn't know what's the problem. And I thought, maybe it's even N.  So the marketing intelligence, which we showed him  was priceless for him.  Like it felt like  he didn't really want to look at the optimization. He was like, yeah. All fine.  We'll just apply it.  Right,  so he was super, super happy. And  we also showed in the morning Assura, where the director client director, even wrote me on chat afterwards. Hey amazing, insights thanks a lo. We will  look at other clients and  same. I think also on Iway. He loved the dashboard. I don't know if he loved the Market insights but he loved the dashboards,  and we now look forward in ZKB, Clara. We  look at every customer,  Klara has it  ZKB has it  same pattern, same problem.  If you wlook at it. Look at the dashboard  scroll at the bottom you see in marketing intelligence  field.  Where we put this information in,  it's all handmade. It's all,  handmade  SQL, queries shit, but it looks beautiful and people love it  and it's really a trend, which we see in the market.  So it's crazy.  The only thing which are not trended is  health  and gaming  there at we don't see change,  all the others are fully affected.  So it's quite fascinating.  So,  on that side,  I think  it's totally underrated what Flavia said, it's like absolute Masterpiece.  Well, how they like it.  And how they loved it. So it's really cool  to get such a feedback because we had very shitty numbers. We only  percent more in scribble  and 80% potential  and he was like we didn't even talk about that.  He was so excited.  So  today I have a couple of internal meetings. Also customer meetings  swisscom. We got a, we got an agreement.  They probably go to essential  until fall  or winter and then go forward, which is fine. That's perfect. And at the same time, we talked with B2C,  GoWago will sign up today and DPV  and Ergo we both have  government's topics process topics. I'm on it,  on top of it. I hope  I mean it should it and now  talking daily with them. So I hope the contracts are going through  and Germany w to be a little bit more  need to know a bit more about this legal process in Germany.  We dknow so far.  Yes.  I think that's all for my site. Ah also investment due diligence, looks very good.  I got the feedback. No, red lights always green.  And the decision might be already next week  if they  take the full convertible,  if they take the full convertible, we probably would take investment only in Fall  to have a better val, so that we don't talk about the 10 million, but more loads of  all the twelve, many, but largely, like, 18 or 20 valuat,  which will be very good because then we can get  bigger chunk of money for Less shares.  You guys all will dilute less  and  the valuation is way better  on the converting point of your shares.  It will be  good better for everybody.  If we  make this happen,  that's all for my side,  the very happy day.  Let's see how it .  And I got spoiled by Silas in the night and I couldn't work anymore after 9, p.m.  By the way, there is a, there is in the support Channel. There's answer from Flavias, answer  Silas, if you can look at that very quickly, if you have five minutes.  Yeah, saw that just shipping it up, right? Yes if you think it's good,  then  Flavia's happy. Thank you. That's all.  Perfect.  Awesome.  And congrats on the all these insects you could deliver with customers yesterday  and happy to hear that. They are happy as well.  And  thanks for the updates  with that. We can close the standup  and the recording hope this was enough Silas.  And.  So question Phil  can even  probably stay. And  can you  describe us  or Philip? And Ivan.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/Users/silas.rudolf/projects/School/MA/experiments/data/nexoya daily standup 2022-03-17.json', 'r') as f:\n",
    "    transcript = json.load(f)\n",
    "speaker_frames = transcript['transcript']['content'][0]['content']\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = []\n",
    "for x in range(len(speaker_frames)):\n",
    "    speaker_name = transcript['speaker_info'][speaker_frames[x]['attrs']['speakerId']-1]['name']\n",
    "    \n",
    "    ARTICLE_TO_SUMMARIZE.append(f'{speaker_name}: ' + ''.join([i['content'][0]['text'] for i in speaker_frames[x]['content']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 964\n",
    "chunked = [''.join(ARTICLE_TO_SUMMARIZE)[i: i + x] for i in range(0, len(''.join(ARTICLE_TO_SUMMARIZE)), x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, batches):\n",
    "    summaries = []\n",
    "    for utterances in batches:\n",
    "        inputs = tokenizer([utterances], max_length=1024, return_tensors=\"pt\")\n",
    "        # Generate Summary\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, min_length=0, max_length=200)\n",
    "        decoded = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        summaries.append(decoded)\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_key = generate(model_keyword, tokenizer_keyword, chunked)\n",
    "sum_phrase = generate(model_keyphrase, tokenizer_keyphrase, chunked)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
